pip install findspark
pip install pyspark
pip install matplotlib
pip install pandas
#- Import các thư viện cần thiết:
import findspark
findspark.init()
from pyspark.sql import SQLContext
from pyspark import SparkContext
from pyspark.ml import *
from pyspark.ml.classification import *
from pyspark.ml.feature import *
from pyspark.ml.param import *
from pyspark.ml.tuning import *
from pyspark.ml.evaluation import *
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import math
#Khỏi tạo các context cho Spark và SparkSQL: 
sc = SparkContext()
sqlContext = SQLContext(sc)
#– Đọc dữ liệu: -
#Dữ liệu đọc từ các tập tin văn bản
#Chuyển tên các chủ đề sang dạng Numeric
#Chuyển đổi sang dạng DataFrame của thư viện Pandas
data = []
topic2id = dict()
i=0
for topic in os.listdir('mini_newsgroups'):
topic2id[topic] = i
i += 1
for doc in os.listdir('mini_newsgroups/' + topic):
with open('mini_newsgroups/" + topic + "/" + doc) as f:
text = f.read()
data.append([text, topic, topic2id[topic]])
# Convert text data to pandas dataframe
df = pd.DataFrame(data, columns=['doc', 'topic', 'label'])

#– Tạo ngẫu nhiên khoảng 80% tập huấn luyện và 20% tập kiểm thử:
msk = np.random.rand(len(df)) <= 0.8
training = df[msk]
testing = df[~msk]
#- Tạo các DataFrame của Spark từ DataFrame của Pandas:
training_data = sqlContext.createDataFrame(training)
testing_data = sqlContext.createDataFrame (testing)
#– Phân bố số lượng văn bản trong các chủ đề thuộc tập huấn luyện:
topic_dist = training_data.groupBy("topic").count()
fig, ax = plt.subplots(1)
topic_dict = dict()
x = []
y = []
labels = []
i=0
for topic in topic_dist.collect():
x.append(i)
1 += 1
labels.append(topic[0])
y.append(topic[1])
ax.bar(x, y)
ax.set_xlabel("Chu De")
ax.set_ylabel("So Luong")
ax.set_title("Du Lieu Mail Theo Chu De")
plt.xticks(x, labels, rotation='vertical')
low = min(y)
high= max(y)
plt.ylim([math.ceil(low-0.5*(high-low)), math.ceil(high+0.01*(high-low))])
rects = ax.patches
# Now make some labels
for rect, y_i in zip(rects, y):
height = rect.get_height()
ax.text(rect.get_x() + rect.get_width()/2, height, y_i, ha='center', va='bottom')
plt.show()
#– Phân bố số lượng văn bản trong các chủ đề thuộc tập kiểm thử:
topic_dist = testing_data.groupBy("topic").count()
fig, ax = plt.subplots(1)
topic_dict = dict()
x = []
y = []
labels = []
i=0
for topic in topic_dist.collect():
x.append(i)
i += 1
labels.append(topic[0])
y.append(topic[1])
ax.bar(x, y, color='r')
ax.set_xlabel("Chu De")
ax.set_ylabel("So Luong")
ax.set_title("Du Lieu Mail Theo Chu De")
plt.xticks(x, labels, rotation='vertical')
low = min(y)
high= max(y)
plt.ylim([math.ceil(low-0.5*(high-low)), math.ceil(high+0.01*(high-low))])
rects = ax.patches
# Now make some labels
for rect, y_i in zip(rects, y):
height = rect.get_height()
ax.text(rect.get_x() + rect.get_width()/2, height, y_i, ha='center', va='bottom')
plt.show()
#- Tiền xử lý các văn bản sử dụng, sử dụng Logistic Regression để huấn luyện và đưa vào Pipeline:
tokenizer = RegexTokenizer(inputCol="doc", outputCol="words", pattern="s+")
remover = outputCol="words") Stop Words Remover(inputCol=tokenizer.getOutputCol(),
hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol="features", numFeatures=150000)
Ir = LogisticRegression(maxIter=20, regParam=0.1)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

model = pipeline.fit(training_data)
#- Lưu model:
cvModel.save("/home/hadoopuser/spark-3.1.2/python/model_muaban")
#- Load model từ nơi lưu trữ để sử dụng:
model = Cross ValidatorModel.load("/home/hadoopuser/spark-3.1.2/python/model_test")